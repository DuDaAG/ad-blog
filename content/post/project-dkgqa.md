---
title: "Deep Knowledge Graph Question Answering"
date: 2024-03-29T14:18:02+01:00
author: "Elias Kempf"
authorAvatar: "img/project-dkgqa/gopher.png"
tags: ["wikidata", "sparql", "kgqa", "llms"]
categories: []
image: "img/writing.jpg"
draft: true
---


## Content

1. [Introduction](#introduction)

2. [Problem definition](#problem-definition)

3. [Approach](#approach)

    - [Training pipeline](#1-training-pipeline)

    - [Initial experiments](#2-initial-experiments)

        - [The T5 model family](#2-1-the-t5-model-family)

        - [Working with Wikidata IDs](#2-2-working-with-wikidata-ids)

    - [Refinements](#3-refinements)

        - [Natural language entities](#3-1-natural-language-entities)

        - [Causal language modeling: Phi-2 and Mistral-7B](#3-2-causal-language-modeling-phi-2-and-mistral-7b)

        - [LoRA and half precision](#3-3-lora-and-half-precision)

5. [Results](#results)

    - [Finetuned models](#1-finetuned-models)

    - [Evaluation](#2-evaluation)

        - [Text-based evaluation](#2-1-text-based-evaluation)

        - [SPARQL-based evaluation](#2-2-sparql-based-evaluation)

6. [Limitations and future work](#limitations-and-future-work)

7. [References](#references)

## Introduction

Large language models (LLMs) have gained much popularity in recent years. They demonstrate impressive capabilities for generating high-quality text and providing quasi-human-like chat-based interactions. Modern LLMs can also be employed for question answering (QA) and will produce reasonable results most of the time. However, we face two obstacles when generating answers to questions with LLMs, i.e., actuality and factuality. For example, answering a question like "Who is the president of the USA?"  requires relatively recent information and an LLM trained five years ago, no matter how powerful, will not be able to give a well-founded answer. Similarly, LLMs tend to provide unfactual answers when queried about information not covered in their training data. Therefore, answers directly generated by an LLM are untrustworthy without prior verification. These limitations make the standalone application of LLMs as question answering systems difficult. However, there is great potential for incorporating LLMs as components of QA systems.

This project presents a method to integrate LLMs into a knowledge graph question answering (KGQA) system. We use an LLM to "translate" questions formulated in natural language into corresponding SPARQL queries over the Wikidata knowledge base. Using our previous example, we want to translate the question "Who is the president of the USA?" to the following SPARQL query (omitting prefixes):
```sparql
SELECT ?target WHERE { wd:Q30 wdt:P6 ?target . }
```
We can then obtain answers to our questions by executing the predicted queries using a SPARQL engine that operates on Wikidata, e.g., Qlever. In this project, we use different pre-trained LLMs that are publicly available and finetune them for predicting SPARQL queries on the Wikidata SimpleQuestions dataset [1].

## Problem definition

We will now define the task we want to solve more clearly. We are given a  pre-trained LLM and a list of question-query pairs from the Wikidata SimpleQuestions dataset as input. Questions are formulated in natural language and may contain ambiguities and spelling mistakes. Queries are formulated in SPARQL and contain Wikidata IDs (we again omit prefixes): 
```python
qq_pairs = [
    (question='Who is the president of the USA?', query='SELECT ?target WHERE { wd:Q30 wdt:P6 ?target . }'),
    (question='wher was neil armstrong born?', query='SELECT ?target WHERE { wd:Q1615 wdt:P19 ?target . }'),
    ...
]
```
Our task now is to finetune the weights of the given LLM such that, given a simple question, it generates a corresponding query. We want the model to be able to generalize to new unseen questions while still being as accurate as possible.

## References

[1] Wikidata SimpleQuestions: https://github.com/askplatypus/wikidata-simplequestions

hugo serve -D --bind "::" --baseURL localhost
